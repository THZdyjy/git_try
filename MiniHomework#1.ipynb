{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 机器学习中的优化  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "公式编辑教程https://www.jianshu.com/p/25f0139637b7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题1 (30分）\n",
    "假设我们有训练数据$D=\\{(\\mathbf{x}_1,y_1),...,(\\mathbf{x}_n,y_n)\\}$, 其中$(\\mathbf{x}_i,y_i)$为每一个样本，而且$\\mathbf{x}_i$是样本的特征并且$\\mathbf{x}_i\\in \\mathcal{R}^D$, $y_i$代表样本数据的标签（label）, 取值为$0$或者$1$. 在逻辑回归中，模型的参数为$(\\mathbf{w},b)$。对于向量，我们一般用粗体来表达。 为了后续推导的方便，可以把b融入到参数w中。 这是参数$w$就变成 $w=(w_0, w_1, .., w_D)$，也就是前面多出了一个项$w_0$, 可以看作是b，这时候每一个$x_i$也需要稍作改变可以写成 $x_i = [1, x_i]$，前面加了一个1。稍做思考应该能看出为什么可以这么写。\n",
    "\n",
    "请回答以下问题。请用Markdown自带的Latex来编写。\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (a) ```编写逻辑回归的目标函数```\n",
    "请写出目标函数（objective function）, 也就是我们需要\"最小化\"的目标（也称之为损失函数或者loss function)，不需要考虑正则。 把目标函数表示成最小化的形态，另外把$\\prod_{}^{}$转换成$\\log \\sum_{}^{}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文哲老师推导的损失函数：   \n",
    "$L(w)=\\mathop {argmin}_{w,b} -\\sum_{i=1}^ny_ilog\\sigma(x_iw+b)+(1-y_i)log[1-\\sigma(x_iw+b)]$  \n",
    "改造为矩阵形式（将偏置放到w中）：    \n",
    "$L(w)=\\mathop {argmin}_{w} -[(ylog\\sigma(xw)+(1-y)log[1-\\sigma(xw)]]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (b) ```求解对w的一阶导数```\n",
    "为了做梯度下降法，我们需要对参数$w$求导，请把$L(w)$对$w$的梯度计算一下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial L(w)}{\\partial w} = X^T[\\sigma(XW)-y]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  (c) ```求解对w的二阶导数```\n",
    "在上面结果的基础上对$w$求解二阶导数，也就是再求一次导数。 这个过程需要回忆一下线性代数的部分 ^^。 参考： matrix cookbook: https://www.math.uwaterloo.ca/~hwolkowi/matrixcookbook.pdf, 还有 Hessian Matrix。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\frac{\\partial^2 L(w)}{\\partial^2 w}=$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (d) ```证明逻辑回归目标函数是凸函数```\n",
    "试着证明逻辑回归函数是凸函数。假设一个函数是凸函数，我们则可以得出局部最优解即为全局最优解，所以假设我们通过随机梯度下降法等手段找到最优解时我们就可以确认这个解就是全局最优解。证明凸函数的方法有很多种，在这里我们介绍一种方法，就是基于二次求导大于等于0。比如给定一个函数$f(x)=x^2-3x+3$，做两次\n",
    "求导之后即可以得出$f''(x)=2 > 0$，所以这个函数就是凸函数。类似的，这种理论也应用于多元变量中的函数上。在多元函数上，只要证明二阶导数是posititive semidefinite即可以。 问题（c）的结果是一个矩阵。 为了证明这个矩阵（假设为H)为Positive Semidefinite，需要证明对于任意一个非零向量$v\\in \\mathcal{R}$, 需要得出$v^{T}Hv >=0$\n",
    "请写出详细的推导过程：  \n",
    "https://www.zhihu.com/question/301865822/answer/529951144"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// TODO 请写下推导过程\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题2 （20分）\n",
    "证明p-norm是凸函数， p-norm的定义为：\n",
    "$||x||_p=(\\sum_{i=1}^{n}|x_i|^p)^{1/p}$  \n",
    "https://blog.csdn.net/itnerd/article/details/82957702\n",
    "\n",
    "hint: Minkowski’s Inequality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// TODO: your proof\n",
    "不会，待老师解答。一点一点掌握矩阵求导手册。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题3 （20分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在第一次课程中，我们讲了在判定凸函数的时候用到一项技术：second-order convexity， 也就是当函数f(x)在每一个点上twice differentiable, 这时候我们就有个性质： f(x)是凸函数，当且仅当 f(x)的二阶为PSD矩阵（半正定矩阵）。 请在下方试着证明一下此理论。 http://www.princeton.edu/~aaa/Public/Teaching/ORF523/S16/ORF523_S16_Lec7_gh.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// your proof of second order convexity  \n",
    "见此链接：https://blog.csdn.net/tanghonghanhaoli/article/details/88110309\n",
    "(1)必要性证明  \n",
    "    根据泰勒级数展开式，对于较小的实数$\\lambda$>0,我们可以得到：  \n",
    "    $J(x+\\lambda{d})=J(x)+\\lambda\\nabla{J}(x)^Td+$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 问题4 （30分）\n",
    "在课堂里我们讲过Transportation problem. 重新描述问题： 有两个城市北京和上海，分别拥有300件衣服和500件衣服，另外有三个城市分别是1，2，3分别需要200，300，250件衣服。现在需要把衣服从北京和上海运送到城市1，2，3。 我们假定每运输一件衣服会产生一些代价，比如：\n",
    "- 北京 -> 1:  5\n",
    "- 北京 -> 2:  6\n",
    "- 北京 -> 3:  4\n",
    "- 上海 -> 1:  6\n",
    "- 上海 -> 2:  3\n",
    "- 上海 -> 3:  7\n",
    "\n",
    "最后的值是单位cost. \n",
    "\n",
    "问题：我们希望最小化成本。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(a)``` 请写出linear programming formulation。 利用标准的写法(Stanford form)，建议使用矩阵、向量的表示法。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// your formulation\n",
    "\n",
    "min object = $CX^T$  \n",
    "其中$C =(5,6,4,6,3,7),X = (B1,B2,B3,S1,S2,S3)$  \n",
    "constraint:  \n",
    "$A_{eq}X=b_{eq}$  \n",
    "$A_{ub}X\\le b_{ub}$   \n",
    "$X\\ge0$  \n",
    "其中：  \n",
    "$$A_{eq}=\\left(\n",
    "\\begin{matrix}\n",
    "1 & 0 & 0 & 1 & 0 & 0 \\\\\n",
    "0 & 1 & 0 & 0 & 1 & 0 \\\\\n",
    "0 & 0 & 1 & 0 & 0 & 1 \\\\\n",
    "\\end{matrix}\\right)\n",
    "$$\n",
    "$$b_{eq}=\\left(\n",
    "\\begin{matrix}\n",
    "200,300,250\n",
    "\\end{matrix}\\right)\n",
    "$$  \n",
    "$$A_{ub}=\\left(\n",
    "\\begin{matrix}\n",
    "1 & 1 & 1 & 0 & 0 & 0 \\\\\n",
    "0 & 0 & 0 & 1 & 1 & 1 \\\\\n",
    "\\end{matrix}\\right)\n",
    "$$\n",
    "$$b_{ub}=\\left(\n",
    "\\begin{matrix}\n",
    "300,500\n",
    "\\end{matrix}\\right)\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(b)``` 利用lp solver求解最优解。 参考：\n",
    "https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.linprog.html\n",
    "    或者： http://cvxopt.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your implementation\n",
    "c = [5,6,4,6,3,7]\n",
    "Aeq=[[1,0,0,1,0,0],\n",
    "     [0,1,0,0,1,0],\n",
    "     [0,0,1,0,0,1]]\n",
    "beq = [200,300,250]\n",
    "Aub = [[1,1,1,0,0,0],\n",
    "       [0,0,0,1,1,1]]\n",
    "bub = [300, 500]\n",
    "from scipy.optimize import linprog\n",
    "res = linprog(c, A_eq=Aeq, b_eq=beq, A_ub=Aub, b_ub=bub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     con: array([3.19494205e-07, 4.80853942e-07, 4.00174116e-07])\n",
      "     fun: 3049.999995512041\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 7\n",
      "   slack: array([4.88704018e-07, 5.00000007e+01])\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([5.00000000e+01, 3.80939619e-08, 2.49999999e+02, 1.50000000e+02,\n",
      "       2.99999999e+02, 1.15679510e-07])\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```(c)```: 试着把上述LP转化成Dual formulation，请写出dual form. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "// your formulation  \n",
    "L(X,$\\lambda,\\nu$)= $CX^T + \\lambda(A_{ub}X - b_{ub}) + \\nu(A_{eq}X - b_{eq})$  \n",
    "其中 $\\lambda\\ge0,\\nu\\in R$  \n",
    "Dual  \n",
    "$\\sup_{\\lambda\\ge0,\\nu\\in R}(\\inf_{X}(L(X,\\lambda,\\nu)))$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
